# -python-BAT-
现在BAT笔试和面试非常容易出现大数据的问题。例如：对5亿个整数进行排序，并输出到文件中。有5亿个ip，找出出现次数最多的ip等等，这些问题的共同特点是数据量较大，并且常规的读入内存再处理行不通，或者即使内存足够大，采取暴力的方法行不通。
针对常见的BAT公司中的大数据面试和笔试问题，列出解决思路，并使用python来实现。

例子中的代码都是基于python2.7.10

## 问题1：海量日志数据，提取出某日访问百度次数最多的那个IP。

## 问题2：寻找热门查询，300万个查询字符串中统计最热门的10个查询。
## 问题3：有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。
## 问题4：海量数据分布在100台电脑中，想个办法高效统计出这批数据的TOP10。
## 问题5：有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排##序。
## 问题6：给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？
## 问题7：在2.5亿个整数中找出不重复的整数，注，内存不足以容纳这2.5亿个整数。
## 问题8： 100w个数中找出最大的100个数。

## 问题9：100亿个数字，如何进行排序？

- [如何给100亿个数字排序?](https://blog.csdn.net/zy_281870667/article/details/52763287)

1. 先拆分为小文件，比如1000个文件，采用hash映射的方式，然后分别对每个小文件进行排序，然后再将这些小文件逐步进行归并排序，每次从堆中选出最大的元素情
    主要在这里面该如何考虑下归并排序，类似于我们的K个链表归并排序，这就知道了哈。
    
    - 首先遍历1000个文件，每个文件里面取第一个数字，组成 (数字, 文件号) 这样的组合加入到堆里（假设是从小到大排序，用小顶堆），遍历完后堆里有1000个 (数字，文件号) 这样的元素
    - 然后不断从堆顶拿元素出来，每拿出一个元素，把它的文件号读取出来，然后去对应的文件里，加一个元素进入堆，直到那个文件被读取完。拿出来的元素当然追加到最终结果的文件里。
    - 按照上面的操作，直到堆被取空了，此时最终结果文件里的全部数字就是有序的了。
    
 一个32G的大文件，用fopen()打开不会全部加载到内存的，然后for循环遍历啊，把每个数字对1000取模，会得到0到999种结果，然后每种结果在写入到新的文件中，就拆分了
 
